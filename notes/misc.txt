Ideas
-----
-for each example, get n (e.g. 4) images and slice them up into grids (e.g. 2x2). Then restructure the boxes into a single image and have the network output a vector of predictions for which box belongs to which image. Or as a more challenging variation, there must be a way to have it indicate the position of each box within its original image. Maybe we decide to use 2 images and decide that "image 1" is whichever image provides the box that ends up in the upper left corner of thet patchwork image. Or let me try working through an example:

[a1][a2]   [b1][b2]          [a4][b2][b4][a3]
[a3][a4]   [b3][b4]   --->   [a1][a2][b1][b3]   --->   [4, 6, 8, 3, 1, 2, 5, 7]   

Here, we number the original positions from left to right and top to bottom 1 image at a time (e.g. counting would go [a1, a2, a3, a4, b1, b2, b3, b4]). Then the model predicts the position index for each item in the constructed image, once again going left to right and top to bottom (though now there is only 1 image. Perhaps as training goes on and the model learns, we can increase the number of images used to create each example. The general hope is that the model learns to distinguish between the two images and also learns the correct structure within each image. 

Another variation: do the same thing but leave one box blank. We can then add an additional task to generate that box. This means the model must identify that there are two separate images, figure out enough about each of them to get the orientations right and see what each is missing, and then use some "imagination" to fill in the missing area. As a simpler variation, we could simply ask the model to identify which image was missing a patch rather than generate it (e.g. 1 means the image that provided the patch in the upper left corner, 0 means the other image). Again, this starts out as a simple task but if we eventually build up to, say, 8 images split up into 8x8 grids to construct a new 64x64 grid image with 1 piece missing, identifying the missing one may not be trivial (or if it is, surely there's a point where that stops being true: the images are only 160x160 (or 128x128, depending on which part of the github readme is correct) so to take things to an extreme, we surely couldn't predict which image was missing a 1x1 pixel box.

While we're thinking of extremes, is there a way to use every image in every example and would that be beneficial? Is there something we could learn about the dataset as a whole this way that we don't get by looking at images 1 at a time? Perhaps each example can be a grid where each box comes from a different image. We then have to predict which position from the original image each box came from (e.g. lower left). They could either all be from the same position, in which case we can output a simple predicted class, or they could all be different in which case we need to predict a grid of classes.

-simpler approach: apply 1 or more of n different transformations. Output vector of predicted classes (consider; could be single or multi-class) for which transformation was applied to each image. But would this learn more about the nature of transformations than the nature of the images?

-Divide each image into 2x2 grid. Apply transformation independently to each segment (e.g. rotate the top left by 90 degrees, top right by 180 degrees, bottom left by 0 degrees, etc.). Then predict the amount each segment was rotated.

-Divide n (e.g. 4) images into 2x2 grids, select 1 box from each. Rotate 3 of them by 1 set angle (e.g. 90 degrees) and the last 1 by a different angle (e.g. 180 degrees). Predict the image that was rotated a different amount than the others. Obviously for this to work they should all be a multiple of 90 (maybe there's a way to avoid that but for the time being that seems reasonable). Idea: predicting rotation already requires some knowledge about what is in the image, but this makes things a bit harder by only showing us part of the image. I'm also thinking that combining images this way might force the model learn something about the relationship between images.
    -variant: instead of rotation being the defining factor, we choose a position in the 2x2 (e.g.) grid. So we might have three "top right" images and 1 "bottom right". 

7/31 - Planning
---------------
Deadline: 9/30/20 for part 1 (image wang + fastai2 experiment). I suspect this will get done a lot faster and I can move on to experiment with additional methods or other datasets/libraries. However, in the interest of removing time pressure I'm setting aside a lot of time to play with this.

Final Product: 1 or more of the following:
    -Just a standard project with notebooks and py files as needed.

    Optionally, this could include 1 or more of the following:
    -Blog post on using fastai, SSL, etc. Probably will skip this though.
    -App that visualizes something about the self-supervised model hidden states? Probably skip this too. I like making apps but I just spent a ton of time on streamlit in my last project and this is meant to be more about trying stuff out in torch/fastai.

Concepts: 
    -self-supervised learning
    -optional: might be cool to try some generative models and see if they could help on this task somehow. VAE would be fun.
Tech: 
    -CometML - Set up experiment-monitoring environment. This was a "nice to have" last time but this time I'm considering it a requirement.
    -FastAI - This is another point that moved from "nice to have" to "requirement". If I like this kind of project, I can try pytorch lightning or revert to incendio next time, but for the sake of clarity I want to make this very simple: part 1 at least must use fastai.
Requirements:
    -Implement >=1 custom self-supervised learning method on imagewang and evaluate how it transfers to a supervised task. Ideally, I'm envisioning a longer string of self-supervised experiments, but I'm not locking that in here because I think I might prefer to do that on text (and possibly with a different library). I'm keeping the scope very limited here: imagewang, fastai, an idea, and we'll see how it goes. For the record, the idea doesn't actually have to be new or innovative: it should be slightly more original than directly copying a paper/blog post, but it's fine if someone else has done something similar or even identical. I don't need to read a ton of papers to make sure this is some novel idea, I just want to be "drawing" rather than "coloring between the lines", so as long as I'm being forced to make my own decisions, it counts.

    -Build at least 2 custom components in fastai2. That's pretty open-ended, but it would be nice to get a sense of the mid-level API.

    -Experiment tracking (i.e. comet.ml). This should be an easy one to check off but I'm requiring it to make sure I don't let it slide.

Pre-Mortem:
    What could go wrong?

    -turns out fastai's high level API is so perfected for this task that I don't really have to do anything custom.
        [SOLUTION] I set a requirement for minimum number of custom components, so even if I CAN get away with this in theory, I've already ensured that I won't give myself credit for it.
    -my pre-training task doesn't work at all. 
        [SOLUTION] This would be disappointing but not overly surprising. I think it would still be acceptable. One smalll variation to the requirements can be that I have to get something that at least mildly works. Or at least ensure that the model is training without obvious bugs - I don't think I can be sure that the pre-training task will be useful since I'm not sure how hard that is to come up with.
    -I end up just following along with a blog post and not trying anything original.
        [SOLUTION] This wouldn't fulfill my requirements so I think we're okay. To be clear, I could start with that, but I'd still need to do something new afterwards. Anyway, I think I'll be okay: I found a nice blog post on SSL w/ fastai2 but it uses a different dataset and a different, simpler pre-training task than what I have in mind. It should provide a good guide without preventing me from trying new things.
    -I find out that I don't like self-supervised learning or experimentation in general 
        [SOLUTION] I guess if this happens it is what it is, but it seems pretty far-fetched given what I've liked and disliked so far.. It would be good to find this out now at least. But I can't really see this happening.
    -I implement my first idea in a day or two, then sort of lose interest. I guess this would be acceptable as long as I wrap things up enough to meet my agreed on requirements. It would be kind of annoying to have to devote energy to picking another project so soon though.
        [SOLUTION] This seems...pretty possible. My hope is that by choosing a pretty small dataset (I think), training will be fast enough that I'll be able to iterate quickly and once I get going, I'll be hooked enough to want to improve on my previous score.
    -My GPU issues return and I get fed up and quit. 
        [SOLUTION] I think I did enough setup on the last project that this is relatively unlikely. One slightly worrying issue is that the last time I tried to start my paperspace machine, the site was throwing weird errors and it never finished setting up. The other concern is that if I have to do a lot of work on GPUs, I can't just keep my work open at all times and it introduces a bigger barrier to starting each night. Maybe the dataset is small enough that CPU training is viable. That actually would be a good challenge: find a method that can be trained quickly on cpu. Realistically, actually achieving this now would be way too ambitious, but it's not a bad thing to keep in mind.

-mixup inspired: Take 2 (or n) images and create a combination of them (e.g. 80% image 1, 20% image 2). Have the model predict what percent is image 1 (always the higher percentage). To do this, it seems like it basically needs to know what the 2 images are of, what they'd look like separately, and whether something looks closer to 1 image or another. (See notebook pg marked SSL 1).
    -recall happy-neuron game: you have to quickly try to extract a few key features from the mashed up image, then look for them in the original images. When the items are recognizable (e.g. bicycle, airplane, etc.) this isn't too hard. When the objects are just scribbles, it's very hard because we can't quickly extract high level features as easily.

-Select either 2 different images OR 1 image and apply a boatload of transformations to it. Then predict which case it is.
    -variation: always select 1 image but randomly choose to split it either vertically or horizontally. Then apply a ton of transformations (e.g. splitting it up into a grid and rearranging the boxes). Then rotate them so the inputs are a consistent shape, and have the model predict if it was vertically or horizontally split.

8/9/20
------
-Failure to learn update: after removing ref to global variable in Unmixer, the model seems to learn something on the scale dataset. However, I think the softmax may not be ideal: 1 of our labels is always zero, but the other two are often somewhat near 0.5. Softmax tends to blow up 1 number near 1. If we use a large temperature, that problem is gone but then it's difficult to predict zero. I suppose we could just use a sigmoid and hope it learns the rest on its own.
-other issue: tried sigmoid and it is learning slowly but it seems to predict 1's for the nonzero images rather than appropriate percentages. Maybe this would be fixed by longer training or larger models, but I'm wondering if the dot product is too simple a comparison step. The net is dealing with several classes of images so it makes sense that a scaled version of the original image would be incredibly similar to the original (i.e. if the encoder is learning to represent 10 different types of animals, a dalmatian and a darkened dalamatian will be represented very similarly.) Maybe we need a larger/more sophisticated head to make this classification (maybe kind of similar to a decoder, although we're not mapping back to the original space. Not sure if this is a similar concept.). Maybe first try existing method on mixup dataset in case the problem is specific to the hue dataset.
    -update: with mixupds, it outputs 1 for everything. Looks like a more involved classification head will be my next experiment.

8/12/20
-------
-suspicious sign: unmixer layer stats remain unchanged after 10 epochs with an enormous LR (e.g. 1000). This is with default encoder and decoder and L1 loss. I think maybe some or all of the losses I've been using are really geared for regression predicting one value, not vector regression.

8/13/20
-------
If loss investigation doesn't fix things, may want to try writing callback to check if model weights are actually changing. Or could continue digging into lightning and try to get tensorboard working to visualize weights and/or gradients.

8/17/20
-------
Tasks ranked from easy to hard (guessing):

1. Scale dataset - classification [SUCCESS]
2. Scale dataset - regression
3. Mixup dataset - classification [FAIL]
4. Mixup dataset - regression

One option is to continue experimenting with Mixup classification to try to figure out why that's not working. Another is to experiment with Scale DS regression. That seems less immediately useful, but it does allow me to work with a dataset that's worked in the past, albeit with different labels/loss/etc.

8/23/20
-------
Brainstorming possible causes of the failure to learn (for now, I'm talking mostly about the scale regression DS):

1. Bug in loss function. 
    -took another look at pairwise loss reduction and everything looks fine to me.
2. Bug in network.
    -# TODO: Network trains fine on scale DS classification so this would be odd. Can try training a tiny, simple network to try to reduce the chances of this.
3. Bug in network last activation.
    -# TODO: looks like I used sigmoid but can try one more time on GPU]
4. Bug in dataset.
    -# TODO: check that labels are formatted appropriately for chosen loss function.]
5. Task is too complex.
    -# TODO: try regression task with smaller n. Or brainstorm other simple alternative tasks. However, scale DS is already supposed to be completely trivial. Not sure how much easier I can make it.]
6. LR is far too high/low.
    -# TODO: If I can make it compatible with fastai, I could use their LR finder. Or the same with lightning.]
7. Gradients are exploding/vanishing.
    -# TODO: Run tensorboard (or maybe log in cometML). Not sure what the solution to this would be - maybe different initialization scheme?]
8. Activations are getting mixed across batches/dimensions in unintended ways.
    -# TODO: ??? Go through models again I guess? See if there's a tool that can catch this more effectively.]
9. Multiple calls to encoder before loss.backward() is screwing up computation graph.
    -# TODO: Think. Classification model does this too so I don't know why it would be different here.]
10. Feature scaling makes it hard to find the original scalar values.
    -# TODO: Feel like this should be learnable regardless. Check if I scale to 0-1 before or after the "augmentation"-esque scaling.]
11. Sampling distribution chooses weights that are too extreme/not extreme enough.
    -# TODO: try different distribution or different parameters of same distribution.]
12. Dataloader problem (e.g. batching images incorrectly).
    -# TODO: Don't think this is happening but can check again.]
13. "class" imbalance: 2 "positives" and 1 "negative" for each example
    -#TODO: Try n that produces equal numbers of positives/negatives? But maybe it will just switch from predicting ones or .67 to .5.
14. Could training on a tiny subset somehow be causing problems?
    -# TODO: Seems unlikely but maybe by giving more examples to train on it will be easier for the net to learn? Still should be able to overfit regardless though, so I don't think this explains it.]
15. Small batch size somehow causes problems?
    -# TODO: Try training on larger bs. Can't see why this would help though.
16. Network too small
    -# TODO: Can't imagine this is necessary for scale DS (very simple task), but try training an ENORMOUS net.
17. Getting stuck in local minimum
    -# TODO: Use Karpathy's trick of initializing bias of final layer.
18. Mish implementation is broken/mish is generally inappropriate for this type of model.
    -# TODO: try something built in (e.g. leaky relu)

online suggestions:
-# DONE: train on random noise. See if learning looks any different.
    [UPDATE: training looks the same. Something is very wrong - hoping batch norm will help.]
-# TODO: look at histograms of weights. Look out for very large weights and severely non-normal distributions.
-# TODO: gradient clipping to prevent exploding grads.
-# TODO: in lieu of LR finder, can try multipling the LR by 10 until I get NaNs. Then try dividing by 10 until I get no learning.
-# TODO: network predicting constant means it is effectively outputting the bias (or bias+1). Neurons are activated all/none of the time. Sounds like better initialization might fix this.
    -Or maybe add batch norm to classification head.

8/27/20
-------
Observations (examining unmixer with mlphead; all acts are leaky relu):
-outputs of encoder are ~50% negative (sounds good), though the dist is heavily right skewed (negatives are tiny bc of leaky relu)
-concat pool makes ~2% negative. Maybe makes sense because this involves a max pool. Quantiles have shifted to be a bit larger but aren't huge.
-multiplication maintains similar negative % but now some acts are enormous (near 100).
-fc1: back to ~50% negative. Magnitudes are still pretty big though, -50 to 50.
-batch norm returns things roughly to normal. Still ~50% negative, magnitudes -2 to 2.
-leaky relu maintains negative %, negative magnitudes are small now.
-final fc layer: 0% negative, pretty small range (.28 to .5).

Thoughts: Kind of seems like it's not a matter of the network being incapable of predicting negatives, it just doesn't. Wonder if larger/deeper MLPHead would help: maybe w/ batch norm, we're putting a lot of responsibility on the last layer which isn't huge at the moment. Or rm last bn layer, but then our scale is a problem. Maybe should rm that but add bn layer after pool and/or multiplication.

8/30/20
-------
-another idea: maybe calling encoder multiple times is screwing up the computation graph? Vaguely recall some kind of retain_graph or retain_gradients function/property/kwarg of some kind, can look into that. Or try rebuilding implementation that reshapes batch and calls encoder only once.
-consider: is there a way to train and evaluate encoder separately from classification head (maybe VAE)? Want to narrow down where the problem is.

8/31/20
-------
-realization: probably not ideal to use cosine similarity outputs directly as logits. Should have some kind of layer(s) to map from cosine sim to logits. However, is this compatible with standard FC layer? Each value already corresponds to a different pair of images so we may need to make sure we don't mix weights that shouldn't be mixed. Technically, scores in the same row are not independent though: due to how we set up the task, one weight being higher leaves less "available" weight for the other images.

9/1/20
------
-ideas from talk with Mike: 
    -try swapping in pre-trained encoder again to rule out model as a problem. 
    -Try log loss (have to think about how this would work. Very similar to contrastive loss I think.). 
    -Train encoder separately first (VAE? or Mike’s suggestion to forego mixup at first and just have the one target image and 3 source images, where 1 of those sources is identical to the target). [UPDATE: don't think this is useful. Batch norm aside, model is mostly deterministic as an encoder. But that actually could be useful in a way: if my logic in ClassificationHead is okay, this should be a spectacularly easy task. If it can't learn that two identical encodings are similar, then what are we even doing here?]
    -See if we can simplify this to a binary classification task to use built in loss and rule that out as an error source.

-even on scale classification which learns very well, grads are very close to zero. Not sure if this means that's a normal magnitude or if the task is just so easy that it doesn't matter, but the mixup task is hard enough to need a normally functioning net.
-looked online and grad magnitudes <.05 (and even under .005) seems normal, though some people got a few grads in the 2-10 range as well.
-looked at grads for scale regression random noise and they look very similar to the real dataset, even though that appeared to work :0 . May need to train on larger scale dataset to make sure it can actually learn (i.e. look at val stats, not just ability to overfit train set).
-looked at grads for mixup classification again and they're actually pretty big. Not sure what to make of this. Tried smaller LR but this didn't help.
-tried pre-trained mobilenet on mixup classification. First run showed a little more variance in outputs but still not learning anything useful. Second run showed similarly low variance outputs to what we've been seeing.

-starting to wonder again if the encoder implementation may be a problem: we only call backward once per forward pass. But weights don't change between backward calls. Still seems worth reworking the batch-reshaping encoder and seeing if the problem disappears. 

9/3/20
------
Observations

-encoder does output the same vector for identical image inputs (as expected)
-all latent vectors are pretty similar in the 1 example I checked. 2 possible explanations:
    -These are all dogs. Maybe if we threw in a goose, it would be more different.
    -Maybe the problem is that I've been training everything at once. Could train encoder alone first, but now that I think about it, that's basically what using the pretrained encoder does. So the key problem may not be that the encoder is bad. Maybe the encoder weights make up such a big proportion of the network that the head has trouble learning. Could try loading pretrained encoder, freezing it, and training just the head.
    -Bigger MLPHead?
-As expected, cosine similarity is highest for the identical image pairs, but the difference isn't as big as I'd have expected. Again, this could be because they're all dogs.
    -Not clear to me how effective the ElementwiseMult is as a similarity measure between two vectors. Seems like it should be fine (we're holding 1 tensor constant so differences are all due to the second tensor). Or is that the problem? Could try L1 or L2 distance. That could be with a reduction to get a scalar like cosine similarity, or without a reduction to get a tensor like ElementwiseMult.

9/7/20
------
-other possibilities to keep in mind:
    -try using Fastai classification head
    -try to reformulate mixup task a little bit to work with triplet loss (or define a slight variant that works with anchor+3 images)
        -see Multiclass N-pair Loss (Sohn 2016 paper). Blog post here: https://towardsdatascience.com/contrasting-contrastive-loss-functions-3c13ca5f055e
        -same blog post as above suggests that the alternate contrastive loss formula I came across is actually NT-Xent loss (Chen 2020 paper). This is implemented in SimilarityHead if we use SimilarityHead with no mlp layers and NLLLoss.
    -idea from adam bielski's siamese network repo: create "test mode" in datasets that sets predefined triplets. I think this is similar to my recent approach of trying to remove all randomness in mini batch creation when training on a subset. (repo at https://github.com/adambielski/siamese-triplet/blob/master/datasets.py)

9/11/20
-------
Batch size experiments

bs: 256
GPU memory (mobilenet encoder): ~6,300 / 16,300 MB

bs: 512
GPU memory (mobilenet encoder): ~11,900 / 16,300 MB
           (img_wang encoder):  ~4,900

bs: 1024
GPU memory (img_wang encoder): ~7,000 (fairly long stretches of low GPU utilization)

9/12/20
-------
DataLoader num_workers experiment notes

-With bs=64, looks like time to load data decreases super-linearly initially but quickly becomes sublinear (i.e. something like T=k/n). Note this does not yet involve putting data on the GPU. Increasing from 1 to 2 workers yielded ~70% speedup, 2->4 yielded another ~50%, 4->8 yielded another ~30%.
-With bs=512, 4 workers took about as much time per epoch (again just loading data on the cpu) as 2 workers with bs 64. I think this is because there's a high time investment to load the first batch (not sure if subsequent batches are affected).
-None of these used much CPU memory relative to what's available. Will have to see what happens when we get the gpu involved.

-
bs: 64
num_workers: 8
Epoch train time: 9s
CPU: 3,050 MB
GPU: 2,050 MB

bs: 256
num_workers: 8
Epoch train time: 9s
CPU: up to3,400 MB
GPU: 6,293 MB

GPU utilization mostly pretty high for bs 128 and 512 as well. bs 512 adds a second per epoch though.

9/13/20
-------
pin_memory experiments

-Doesn't seem to make much of a difference in speed, maybe even adds 0-1 sec/epoch.

-Other task ideas:
    -use albumentations MedianBlur or JpegCompression transforms. Then use SiameseNet to predict whether a pair of images has the same source image or if they were randomly paired. Thinking this forces the net to focus on high level features since details aren't available in the blurred versions.
    -similar: apply lots of transforms to try to change the source image a ton. We want hard pairs (or triplets), so a simple rotation (for example) may not be as effective as {rotation + blur + noise + rgb shift + vertical flip}.

9/18/20
-------
S01 error (I think my auto calculation of encoder output dim isn't working correctly):

'RuntimeError: non-empty 3D or 4D (batch mode) tensor expected for input
