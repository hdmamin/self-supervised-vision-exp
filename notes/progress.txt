7/30/20
-------
Progress: Brainstormed ideas for intial approaches to imagewang.

Plans: Write detailed pre-mortem. Create github repo and push current files.

7/31/20
-------
Progress: Wrote plans and pre-mortem. Created github repo and pushed files. Created new Comet.ml project. Played a few games at http://www.happy-neuron.com/ to generate ideas for visual reasoning and get other ideas for learning.

Plans: Try loading a few sample images to get a feel for dir structure and get a sense of what's in our dataset, how big each split is, etc. Start following along with epoching blog (leaning towards starting with my mixup-inspired task).


8/1/20
-------
Progress: Fiddled around a bit with fastai's datablocks and lower level API but concluded pytorch is more intuitive, more fun, and should work fine. As long as I make everything work with the fastai training loop I think I'll consider that requirement met. Started working on torch dataset, mostly consisting of transform that mixes 2 images (functionality isn't very generalized yet).

Plans: Fix tensor resize issue in dataset. Once I have things working in pure torch, maybe go back and take another stab at the fastai way.


8/2/20
-------
Progress: Added image resize using native Pillow method. Batching wasn't collating right so struggled a bit with custom collate_fn's, but ultimately changed dataset getitem to make this easier. Started checking how to re-separate batch into the desired parts for the model (i.e. need x_constructed, x_a, x_b, x_c, where each x is shape (bs, channels, h, w).

Plans: Start working on model. Mostly just make sure batching is set up in a usable way that will allow me to encode each image separately.

8/3/20
-------
Progress: Cleaned up MixupTransform and made it a little more generalizable. Ported data functions/classes to lib and wrote docs. Started very early work on model. Fixed issue loading 1 channel images.

Plans: Take another look at fastai blog post. Consider best way to deal with encoding multiple images (maybe simplest to keep source images horizontally stacked as single tensor?). Work on building encoder.

8/4/20
-------
Progress: Figured out weird dataset/dataloader batching issue and changed it to return each image separately instead of concatenating into 1 giant image (cause: getitem should return like (x1, x2, x3, x4, y), not ((x1, x2, x3, x4), y)). Built out first stab at model arch using a stack of conv blocks.

Plans: Update incendio resblock interface to be consistent with convblock (seems like I'm leaning towards using that over fastai. I guess that wouldn't be so bad - using fastai does risk me glossing over some details, and if this leads to more comfort with torch and a more developed incendio library, that would be good too). Generalize UnmixerModel to accept resblock. Also consider: did I make a mistake by restructuring the batch this way instead of applying an encoder n+1 times? This is just the pre-training task, eventually I'll want to input only 1 image at a time. Maybe it's okay, just need to give this some thought.

8/5/20
-------
Progress: Gave some thought to the issue of processing all images at once and concluded that it's probably fine - when transferring, I can just pull out self.conv and self.pool and add a new head in forward(). Updated incendio resblock and convblock interfaces to be more similar. Updated UnmixerModel interface to allow the addition of 1 or more resblocks after the initial conv blocks (realized it might be unwise to use res blocks from the start since we can't increase the channel dimension) and generally did a pretty significant model overhaul. Wrote wrapper to get train/val datasets and dataloaders. Wrote dataset shuffling callback. Fixed a few incendio bugs and started training on a tiny dataset for debugging purposes.

Plans: Investigate empty history plot bug from MetricHistory callback. Investigate if model is learning anything (early results look pretty random) and if not, try to determine the cause (e.g. is LR just too high or is something fundamentally wrong with the model, e.g. mixing data across batches or something).

8/6/20
-------
Progress: Found cause of history plot bug and fixed it (at least I think - haven't thoroughly tested cases where there are more metrics). Continued trying to overfit on tiny batch with smaller LR and more epochs and still failed. Think there must be a problem with the model, probably mixing up images or examples somewhere along the way. Started working on a model variation that foregoes complicated batching and just puts each image through the encoding layers separately.

Plans: Finish up simple encoder and see if that also fails to learn on a tiny batch.

8/7/20
-------
Progress: Finished simpler encoder. Found and fixed bug in dataset that was giving it the wrong length. Found and fixed bug in model forward where I was squeezing the batch dimension if it was 1. The model continues to fail to learn on even a tiny subset.

Plans: Try swapping in a large pre-trained model to see if that can learn. This doesn't count for the self-supervised task but it's a way to see if my model is the problem. If this still doesn't learn, maybe it's something with the dataset or dataloader or loss function. Or could the task just be too hard when training a model from scratch? That seems unlikely, unless maybe we just need a bigger model? 

8/8/20
------
Progress: Tested pre-trained mobilenet encoder on tiny subset and that also failed to learn. Built model wrapper that allows us to pass in an encoder network (e.g. mobilenet) and applies the additional pooling, dot products, softmax, etc. at the end. This should make it easier to swap different architectures in and out. Wrote 2 new datasets to allow more testing to determine where the bug is.

Plans: Test model on ScaleDataset and see if that is able to learn. If not, maybe there's a problem with my dot product to softmax steps. Clean up nb2 a bit.

8/9/20
------
Progress: Updated get_databunch() function to allow us to choose which dataset type to create and added extra defaults to shuffle train set and drop incomplete batches. Found bug where Unmixer referenced global var and removed it. Added sigmoid option as final activation since softmax seems somewhat problematic here. Trained a bit on scale dataset and model shows some ability to learn, though the sigmoid model still outputs ~1 for the nonzero images. Still not really learning on mixup task. I think my dot product layer needs to be fleshed out into a more powerful classification head.

Plans: Refactor Unmixer to accept both the encoder and decoder as separate modules and build current functionality into a new decoder class. Maybe try a couple other decoder options (probably just start with some additional linear layers of some sort - have to think about exact strategy and implementation).

8/10/20
------
Progress: Heavy refactoring: Unmixer now accepts an encoder and a classification head. Built abstract classification head class, refactored dotproducthead into its own class, and added MLPhead class. Updated SmoothSoftmax class from attentive pooling experiments. Tried it out briefly and I THINK everything looks okay but need to check more. Trained on subset with MLP head and sigmoid activation and net now predicts all zeros :( .

Plans: Clean up notebook - it's gotten out of hand. Think it may be nearing the time to port some functionality to lib and start a fresh nb. Maybe first experiment a tiny bit more to see if a softmax last_act works better.

8/11/20
------
Progress: Tried a couple variations of refactored models on ScaleDataset and now the model completely fails to learn (guess I should have kept that example that was working). Ported all the models to lib and wrote some documentation, though skipped the more parameter-heavy constructors: it's possible we'll need to refactor more so I'm holding off on that. Part of the goal here was just to put everything in pycharm to see if it found any dumb global variable mistakes (it did not).

Plans: Start new clean notebook. Go through notes and try to reproduce the 1 or 2 examples that sort of worked. If necessary, create an even simpler version of ScaleDataset and see if I can get that to work (e.g. n=1 so we just have the original image and 1 scaled version? Have to see if this is compatible with current implementation).


8/12/20
------
Progress: Found and fixed bug in DotProductHead where last_act was applied (it's applied again by parent class). Changed Unmixer to inherit from BaseModel. Updated ScaleDataset to allow for n as low as 1. Ran some more experiments and still failed to learn anything - in fact, it's not clear if model weights are even changing. Predictions seem to remain pretty constant, which I initially thought might be caused by one of the models referencing some global variable. Now I'm wondering if I messed up the incendio training loop or optimizer with one of my recent changes. Explored different loss functions a bit and realized these probably aren't really itended for vector regression (may not be handling batches right).

Plans: Investigate whether weights are changing (aggregate stats seem unchanged but maybe specific weights change). Maybe best way forward is to just bite the bullet and write a pure torch loop to see if incendio is the problem. Or maybe I can try Lightning without rewriting anything. I've already scaled data/task complexity way down so next is to rule out incendio bugs.

8/13/20
-------
Progress: Ported functionality to pytorch lightning module and datamodule. Tried creating cometml logger but was getting errors so tried out mlflow logger (still some kinks to work out). Finding it surprisingly hard to find metric histories, especially train metrics - maybe need to explicitly add that to the logger in forward_step(). Early results don't look encouraing though (based on glimpses of loss in tqdm progress bar). This suggests the bug is not with the training loop.

Plans: Continue testing with lightning, adding ways to see training metrics. Try replacing variable LR optimizer with default torch optim to rule out that as a cause. Try replacing model with something SUPER simple. Starting to wonder if it's the loss function, but it may be a good idea to rule out a couple other causes first. Try to get lightning + mlflow + comet to save everything where I want it rather than cluttering up root directory.

8/14/20
-------
Progress: Built new dataModule and model for supervised task. Swapped out repeated squeeze calls in unsupervised model for a PoolFlatten layer. Tested supervised data/model on a subset and I think there's STILL an error (though hard to confirm since lightning seems intent on hiding all metrics).

Plans: Figure out where lightning stores training metrics. Try to identify possible bug in supervised task code (hoping to succeed in overfitting on 1 batch).

8/15/20
-------
Progress: Realized I was using wrong loss for supervised task and swapped in F.cross_entropy. Wrote function to create a subset of a built-in Torch dataset (haven't tested it yet on my custom datasets but I think it's flexible enough that it should be usable with them). Succeeded in overfitting on a tiny subset of the supervised task using Incendio (1 class only), then confirmed training also seems to be working on a slightly larger subset representing most classes (40 row train set). Ported ds_subset function to lib and wrote docstring.

Plans: Now that incendio loop/optimizer seems unlikely to have caused the failure to learn on the unsupervised task, investigate loss function. See if there's something built-in I should be using (maybe pairwise_distance will work?) or write wrapper to L2/L1 loss that handles reductions appropriately. Maybe port some of the recent lightning functionality to lib.


8/16/20
-------
Progress: Added multi-label classification mode to mixup and scale datasets (simpler task: predict which images are sources, not what their weights were). This lets us train in a way that I'm confident is not mis-using a loss function (binary_cross_entropy_with_logits with OHE targets). Confirmed ability to overfit 1 batch using identity last_act, 3 layer encoder, MLP head. Had to restart nb due to autoreload failure and training is now oddly much slower. Maybe I was using dot product head before, but encoder was much bigger. Tried classification mode with MixupDS and model predicted ~.67 for everything.

Plans: Think about what this problem means and research fixes. Or port lightning functionality and write some documentation if I want something less mentally taxing.

8/17/20
-------
Progress: Laid out some thoughts on different approaches to solve problem and decided to work on regression version of ScaleDS. Wrote loss wrapper for pairwise distance and trained a bit in regression mode - model seems to learn a little bit but still struggles to overfit one batch. Maybe need bigger model/more epochs/lower LR? Ported some lightning functionality to lib. Started looking through siamese network repo for ideas on what could be wrong.

Plans: Investigate more to see if scaleDS regression is actually learning (may want to break out GPU for larger/longer training - should still be extremely fast on 1 batch, and it's been getting annoyingly slow on CPU as I increase model size). Less mentally taxing alternative: still tons of documentation to write. Also can look more through siamese network repo.

8/18/20
-------
Progress: Added documentation for datasets and models (lightning modules and data modules are still undocumented).

Plans: Same as today: probably just write lightning documentation and/or read through siamese network repo. If I have more energy, launch larger scaleDS regression run to see if learning is taking place.

8/19/20
-------
Progress: Read a bit about contrastive loss and took first stab at implementing it. Have a functional version and an OOP version, both of which seem to work, but some work remains.

Plans: Read more about contrastive loss variant that uses softmax and cosine distance and try implementing that (maybe make it so there's 1 top level interface but the user can specify distance='cosine' or mode='softmax' or something). If that gets done, could look into adapting this for use on soft targets.

8/20/20
-------
Progress: Read a little about contrastive loss variant. Ended up deciding that with my current setup, this would work best split between a very simple classification head that computes the cosine similarity (or other metric if desired), leaves the temperature scaling and log softmax to the last activation, and uses NLLLoss as the loss function. Ended up refactoring smoothsoftmax to provide a logsoftmax option.

Plans: Update ClassificationHead (and other classes if necessary) to accomodate log variant of smoothsoftmax (realized torch losses expect either logits or logsoftmax outputs). Maybe return to contrastive loss issues (see nb04 todo).

8/21/20
-------
Progress: Updated classification head to allow log softmax. Built contrastiveloss2d variant that seems to be working as I envisioned when comparing 1 image to multiple others. Wrote just enough documentation to remember what I'm doing but it still needs to be cleaned up.

Plans: Options depending on energy level. Easy task: port and document contrastive loss 1d/2d/functional forms. Medium task: work on contrastive loss variant for scalar inputs (maybe this already works? As in works from a python standpoint, no idea if this will work well in practice from an ML standpoint). Harder task: return to the problem of scaleDS regression by training a larger model for longer on paperspace now that wifi's working again.

8/22/20
-------
Progress: Set up paperspace for img wang experiments (pull data, install new packages, etc.). Re-ran some experiments on GPU but not a ton of progress: confirmed that scaleDS classification seems to work okay, but still failing to learn anything remotely useful on the other 3 tasks. 

Plans: Realized similarityHead cannot use NLLLoss (I think this is only available for single label?) so figure out what loss to use for that head (may need to rework head a bit or build custom loss). Check pairwiseloss reduction to make sure it's doing what I think it is - since the classificaiton task works for scale DS, I'm thinking the problem for regression must be either the loss function or how it interacts with the last activation. If this doesn't reveal problems, spend some time considering how to proceed. Is there a way to tell if a task is just impossible for a randomly initialized network to learn on?

8/23/20
-------
Progress: Brainstormed and wrote down all conceivable causes of failure to learn. Did some reading and found a few more possible issues. Updated MLPHead to allow for one possible fix (optional batch norm after linear layers to avoid neuron saturation). Updated incendio with inverse_sigmoid, cometcallback, and is_builtin() (mostly written already but added some documentation and updated kwargs to reflect new callback interface). Wrote function to update a layer's bias, optionally using the inverse sigmoid trick, and added to incendio.

Plans: Most promising strategies to try first (choose whatever sounds most appealing): train on random noise (maybe this could be a callback? Or a different dataset mode?); replace mish activations with something builtin; use batch norm in MLPHead. Simpler task: Add inverse sigmoid trick to one or more classification heads. Careful: given that we have supervised and unsupervised tasks, regression and classification tasks, etc., we need to make sure we implement this in a way that is a. valid, and b. flexible enough to account for all cases, and c. ideally obscured from the user-facing interface.

8/24/20
-------
Progress: Wrote function to generate random noise in the shape of each input tensor and updated all three datasets to have this option. Added inverse_sigmoid bias initialization option to MLPHead (checked the other heads but the ones I've written so far don't have layers with biases). Found some missing documentation and wrote it. Read a little about batch norm - will have to experiment and see how this goes, some advise to skip this in final layer(s) but I suspect the lack of it is causing the issues with my last layer, resulting in the model effectively always predicting the bias.

Plans: Ideally, try a simple experiment or two (new random noise mode, replace mish with leaky relu, or batch norm in MLP head are the tactics to start with). If I really don't want to, could write gradient clipping callback, research more about reasonable values of margin in contrastive loss, or read more through my open github repos or loss articles and try to reduce some of my tab clutter on this project.

8/25/20
-------
Progress: Updated random noise function to clip values at 0 and 1 by default (so more like truncated normal - could always pass in -9e9, 9e9 as bounds if we want more of a true normal). Tried training scaleDS regression on random noise and it performed exactly the same (suggests something is very wrong with training). Fixed bug in MPHead (batch norm 1d needs dimension specified and it should be the the 2nd of 3 dimensions, oddly). Tried training scale regression with batch norm and while training is still noisy (maybe LR too high), model finally learns a bit! Granted, it's still only on 1 batch so I don't know if it's learning anything useful, but at least we're making progress. Tried MLPHead with batch norm on mixup classification and sadly, model still predicts a constant. Tried adding init bias trick as well but this also failed. Fixed bug with bias_trick (last_act accepts value None, not 'none'). Cleaned up some outdated code in MLPHead.

Plans: Option 1: write predict method for trainer and/or basemodel (tired of having to manually map everything to the gpu when I'm using that but not when on cpu). Option 2: examine outputs of encoder and classification head to try to identify where the issue is (somewhere activations must be becoming either huge or tiny). Option 3: just try training with different activation function. Also check to make sure I didn't make any unintentional changes between scale classification and mixup classification.

8/26/20
-------
Progress: Turns out incendio trainer already has a predict method, but I added one to basemodel and refactored the trainer method a bit (also made classification head inherit from basemodel). Tried training mixup classification with leaky relu and relu in place of mish but it didn't help. (Technically the model doesn't exactly output the bias for everything but the range of predictions is still quite small, e.g. .55-.75, and of course they're all the same class). Removed incendio progress bars at end of each epoch (works in notebooks, at least - bars don't show up at all in lab :/ ) and fixed plotting bug in metrichistory callback. Also removed time from incendio logger printouts (all in all, training history is looking cleaner. Still kind of tempted to make everything a column so we get one nice clean table but I'm torn because I remember very specifically wanting to avoid that because with a lot of metrics that can get very messy). Took another look at scale classification vs. mixup classification and I swear they look the same >:( .

Plans: Adjust how MLPHead handles `act` to avoid potential issues with multiple layers referencing same object. Return to issue of examing encoder and head outputs to figure out where the activations are blowing up.

8/27/20
-------
Progress: Decided MLP handling of `act` is fine (no weights anyway so same obj can be reused). Examined activation magnitude and sign after each layer and identified a couple possible issues (concat pool -> element-wise mult causes huge activations; future layers bring them back to normal but grads may have gotten screwed up in the process). Tried adding bn after mult but no difference. Tried removing bn after each fc layer but keeping it after mult but no difference.

Plans: Continue investigating activations by layer. Maybe try training with bigger Head. Maybe look into torch's default activation schemes for each layer and see if we can find something better. Need to refresh my memory on how relu/leaky relu doesn't always cause this problem - seems like chopping off negativs makes it inevitable.

8/28/20
-------
Progress: Read up a bit on initialization schemes (torch already uses a kaiming init variant, seems reasonable enough). Fixed an incendio import bug and cleaned up some old commented out code. Tried recently updated MLPHead on mixup regression task (same result as classification). First stab at gradient logging by subclassing CometCallback.

Plans: Examine results of comet gradient logging more thoroughly. Update callback to store results in a more useful way instead of littering the metrics page with a separate graph for each layer.

8/29/20
-------
Progress: Examined comet gradient logging and switched up functionality: store grad means and stds, then plot and store bar chart at end of training. Found that last layer and some earlier batch norm layers are the only ones with grads that aren't near-zero. Also experimented with scatter plots of grad stats by mini batch but this made it hard to get a quick intuition for how early layer grads compare to late layer grads. Tried much shallower network and the same thing happens. Wondering if this is vanishing grad or if there's some bug causing us to lose grads in the middle of the network.

Plans: Investigate for causes of vanishing grad. Maybe try replacing element-wise mult with cosine similarity. Maybe rename and port CometCallbackWithGrads.

8/30/20
-------
Progress: Adapted SimilarityHead to work in cases not using contrastive loss and tried swapping it in for MLPHead, but mixup regression still failed to learn. Tried this on classification task too but still nothing. Occurred to me that calling encoder multiple times before backward pass could plausibly be messing up computation graph so returned to my first implementation which reshapes the batch to process everything at once. Worked a bit on this but realized it causes some problems with the Unmixer interface and PoolFlatten (outputs rank 5 tensor which isn't compatible with adaptiveavg2d). Ran scale classification task again with gradient tracking to compare what's happening in the working task to the non-working task.

Plans: Examine grads on scale classification and repeat the process on scale regression. Try to identify where in the process things are breaking down for mixup ds. Maybe look more into what is considered "too small" for gradients": even on scale classification task, most grads are pretty near zero, especially for encoder. (Put batch reshaping encoder on hold - regular encoder worked on simpler tasks so I'm not convinced that's the problem and it looks like it will take some time to make the interfaces compatible.) 


8/31/20
-------
Progress: Experimented a little with options for ways to map cosine similarity scores to logits. Ultimately settled on regular FC layer in similarityhead (originally tried MLP head but realized that is really meant for acting on feature vectors while cosine similarity outputs scalars). Built elementwiseMult layer for two input tensors (a bit silly but it was initially to allow us to easily swap between that and cosine similarity before I realized the dimensions differ).

Plans: Return to task of examining grads on scale classification and repeat process on scale regression. Could investigate these further or try new similarityhead implementation.

9/1/20
-------
Progress: Tried tracking grads for 3 of 4 task variants again and found that oddly, grads are BIGGER on mixup classification, which doesn't work, thanon scale classification which does work. Worryingly, grads for scale classification with random noise looks very similar to grads for the real task. Tried pretrained mobilenet on mixup classification task and it also failed to learn anything. Gradient flow looks pretty good here though (looked online and looks like magnitude <.05 is normal). Started building trivial variation to test if MLPHead logic works at all: replace one of the zerod out source images with the new mixed image and a label of 1. Encodings should be identical (I think) so surely this will be easy to learn unless something is seriously broken.

Plans: May need to revise trivial dataset variation a little (see today's progress. Might want to rename attr. Note: consider what type of problem/loss I want to use this with. Maybe try to keep same as real task for consistency. Can always test another binary variant separately - don't change too many variables at once.) Hopefully get this working and try training on a tiny subset.

9/2/20
-------
Progress: Reworked the duplication function to allow us to duplicate the mixed image or a source image. Updated dataset interface to have a single debug_mode param. Wrote rand_choice func (torch equivalent of np.random.choice with limited functionality). Tested that each debug_mode seems to produce what I have in mind and tried training in classification mode in source image duplicate mode. Model still fails to learn - frustrating but I suppose this means I'm slowly narrowing in on the bug. If it can't do the task on images that are literally identical, something is broken.

Plans: Investigate further: see if encoder is at least outputting the same vector for the same inputs. Try debug node where we duplicate the mixup image, though I'm pretty sure that won't work if the source duplicate mode doesn't.

9/3/20
------
Progress: Examined encoder outputs (see notes from misc.txt). Tried using a pretrained encoder and freezing it while training just the head. I think one of these experiments showed a little promise (at least a slightly wider range of probabilities and the positive pair was the highest) but don't think I ever got it to fully work. Tried larger MLPHead since I noticed it was dwarfed by the mobilenet encoder but it didn't help. Tried adding a FC stack to similarity head (cosine similarity seems more reliable than ElementwiseMult) but that did terribly (some variation in probas but the highest pred was not the true pair).

Plans: Continue experimenting with this duplicate task and pretrained model. I think I'm getting closer: the cosine similarity values alone seem like they should be enough to make predictions. Surely the model can learn to predict positives for largest cosine similarity  (especially since it always equals 1).

9/4/20
------
Progress: Tried a few more training runs to no avail. Decided to switch focus to single image input tasks for now - if we can get that working, maybe the multi-image input task will be doable. Tried a few training runs on tiny quadrant dataset. No longer have issue of predicting constant but still failed to overfit batch so far. Huge fluctuations make me think LR is too high.

Plans: Continue trying to get quadrant dataset to work. Consider other 1 image input tasks to try.

9/5/20
------
Progress: Built dataset for single image input binary classification task: patchwork dataset. Decided to pivot here: instead of endlessly banging my head against the wall trying to get the multi-image input tasks to work, I think I need to get a win first and find a simple task that works. Then maybe I can build my way back up to MixupDS eventually, but for now it's not my focus. I'm not giving up on it, I just realized the best way to progress on it in the long term is to use my time effectively to get better at self-supervised learning and I was kind of at a dead end with that. And I decided that if I'm going to switch my focus in the short term, I might as well pick something interesting, and quadrant DS just didn't quite do it for me. Also, it's still multiclass classification (a little more complex than binary) and the task could be kind of hard. Patchwork DS is more visually interesting but maybe (I would guess) a little easier to learn on.

Plans: Assess whether any of my existing model functionality is usable for the new task or if I need to start from scratch. Either way, start building a model or wrapper to make existing stuff work.

9/6/20
------
Progress: Built new model wrapper (equivalent of Unmixer but for single input binary classifier). Fixed a couple small bugs in Patchwork Dataset (still not 100% sure the cloning issue is fixed - keep an eye on that). Ported comet callback. Trained a bit with old Encoder and one layer linear head, then with torchvision encoder, then with frozen torchvision encoder. Some all of these still fail to learn. Though LR was too high, causing wild fluctuations during training, but model still fails to learn after dividing LR by 300.

Plans: Investigate what is going wrong - starting to wonder again if this is an incendio problem. With a frozen pre-trained encoder, all we're doing is training 1 big linear layer. How could this possibly have so much trouble overfitting on a single batch?

9/7/20
------
Progress: Fixed a couple incendio bugs in handle_interrupt() that resulted from updating on_train_end() parameters. Tried training longer on patchwork task (and on smaller batches and dataset subsets) for debugging purposes but this didn't help. Refactored transform functions a bit (and datasets to use the new interface) so that we pass in x and y in the same order they're returned in. Added option to patchwork dataset that lets user choose the percent of images that are positives (contain patches from themselves) and added a debug mode to let us keep the patch position fixed. By setting pct_pos to 1 and using debug mode, we were able to overfit on a tiny subset (2 images each in train and val)! Still haven't learned useful things but at least we're seemingly on the right track to building something functional. Lesson: a tiny subset isn't as tiny as it seems if your task involves any randomness in constructing or augmenting each item. Now that I think about it, Karpathy did say to turn off all transforms. Read through post on triplet loss and pairwise ranking loss (same as contrastive loss) and looked through a few github repos on siamese networks and the like).

Plans: Build larger default classification head than the single layer. Maybe we're ready to try larger runs on GPU. First, spend a little time thinking about how we want to do this: should we train in a notebook, a script, a script that executes a notebook (e.g. papermill); what artifacts do we want to store, either locally or in comet or s3; what directory structure should we have (just dump everything in data/models/v{n}?). 

9/8/20
------
Progress: Added fastai classification head as the default for patchwork binary model (cut off pool and flatten, which I added separately). Tried training this on a subset and it does far worse than the single linear layer, even when turning off dropout (???). Also tried increasing batch size in case it was adding too much noise to the batch norm stats, but still didn't learn anything. Started writing script s01 to prepare for GPU experiments (still skeptical this will work, but the single linear layer easily overfits the subset so it seems time to progress to larger scale training).

Plans: Continue fleshing out s01. Consider running a notebook training run in the meantime - script is slow to build and this would give us some intuition for whether the task learns at all (so far we've been focused entirely on just trying to make the implementation function at all, but this is an entirely separate question of whether it can learn effectively on something besides a tiny subset).

9/9/20
------
Progress: Wrote a couple functions to set all random seeds and/or check if gpu is available. Updated lib requirements and built environment on paperspace. Debugged weird dataset issue (memory location error despite use of clone and detach) and found that it was always happening with the same images, so I just had the dataset delete those paths. Almost got training working on GPU but need to add comet config file.

Plans: Add comet config file and get training going in notebook. While model trains, work on training script s01.

9/10/20
-------
Progress: Got comet configured on GPU. Ran first larger scale experiment with patchwork dataset and it seemed to work decently well so far (granted with a pretrained encoder, and I haven't tried transferring the weights yet, so there's a ways to go. But at least there's signs of progress). 

Plans: Examine results of v0 more in depth. Find what batch size can fit on the GPU. Options: clean up post-experiment code into a "top_mistakes" function, continue writing s01 script, try a run with an untrained encoder (though maybe best to power through the script before doing future experiments).

9/11/20
-------
Progress: Experimented with batch sizes to see what can fit on GPU. Ran v1 experiment with 8x bs which learned more slowly (need to add higher LR and increase dataloader num_workers to speed up training). Wrote some documentation for PatchworkDataset. Examined v0 results a bit.

Plans: Experiment with num_workers to see if that speeds up dataloader like I think it will. Examine v1 results more thoroughly. Maybe get back to work on s01 script.

9/12/20
-------
Progress: Experimented with increasing num_workers with various batch sizes (see notes/misc.txt) and found large speedups (~9s epochs now!). Examined v1 results a bit. Continued fleshing out s01 script - still not quite ready for training but it's getting close. Though as I get farther, I'm now realizing we likely need to train the encoder by itself first before introducing the classification head (v0 avoided this problem by using a pretrained encoder, but of course that defeats the point). Read Chaudhary blog post on self-supervised CV.

Plans: Work on s01 training script more. Maybe try training run without pretrained weights to validate my suspicion that this won't work well. Maybe try loading a larger torchvision model to see how much this affects training times, viable batch sizes, etc.

9/13/20
-------
Progress: Experimented with pin_memory in dataloader but it didn't seem to help. Tried randomly initialized torchvision encoder and, as expected, it learned nothing. However, tried my custom encoder with fastai head and that was fitting the training set pretty well. This was with dropout turned down so maybe if we increase that a bit we could learn something. Also read another blog post on self-supervised learning (not that useful) and revisited epoching's blog post to confirm whether they trained their encoder separately (they did not). Combined unattached code into "top_mistakes" function which showed more evidence of a possible dataset bug.

Plans: Manually examine some images and read through image_wang and/or imagenet documentation to see if there might be duplicates. Otherwise, may need to investigate Patchwork Dataset for possible bugs. Consider adding debug mode: perhaps try adding attributes to each tensor showing their img1 and img2 indices.

9/14/20
-------
Progress: Updated patchwork dataset to store indices of both images as attribute. Manually examined a couple hundred exaples looking for bugs but didn't find any. Realized model "top mistakes" were a more efficient way to find these possible bugs, so loaded one of my trained models to do this. Realized this would require some updates to top_mistake func and I started on this but didn't finish. Did a bit more library refactoring to separate torch_utils and utils as well.

Plans: Maybe update collate_fn to store ds x.idx as batch xb.idx, or consider if there's a better way to do this, since this might make for kind of an annoying interface if I swap in other datasets. Finish exploring the potential bug issue (try to wrap this up today - given than the couple hundred samples I explored showed 0-1 errors, this doesn't seem like a big problem. Even if something is going wrong, it's not common and we can handle some noise. I'm thinking there are just some very similar images (though that only explains the negative labels which look positive; the positive labels which look negative are still a mystery).

9/15/20
-------
Progress: Wrote collate_fn that stores src indices on batches and updated get_databunch to allow custom collate_fn. Found issue where collate_fn sometimes fails to store idx when num_workers > 0 and added warning. Made significant updates to top_mistakes() and used this to check results using the new idx provided by collate_fn. So far not seeing any evidence of bugs: I think there are just some images that look pretty similar. This is kind of a good sign - some of the model's mistakes were on samples that fooled me too.

Plans: Work more on s01 (try to finish?).

9/16/20
-------
Progress: Tentatively finished s01 (still need to test). Added 2 optional transforms to PatchworkDataset (random flip vert and flip horiz). Also had to manually build flip since torchvision acts on pillow images and I don't really want to keep converting back and forth (also messed around a bit trying to get albumentations to work but ultimately decided manual tensor flip was easier). Built RandomTransform class to convert any function to a random transform. Wrote some documentation and fixed a couple old import statements. 

Plans: Maybe add a "random noise" transform. Maybe test out train script to see if there are bugs.

9/17/20
-------
Progress: Buit random noise transform. Created new func_name func in htools and updated basicpipeline with it. Updated randomtransform repr as well but that's not working yet - maybe an autoreload issue? Updated patchwork ds with option to use randomnoise transform.

Plans: Update docstring for patchwork ds with new transform kwargs. Test out to make sure it works as expected. Update s01 with transform kwargs.

9/18/20
-------
Progress: Updated docs for patchwork ds to include new transform kwargs. Updated training script to include transform options. Fixed some ubuntu issues causing fastai import errors. Fixed a bunch of bugs when I tried to run train script s01. Added func to lib calculating encoder output dim.

Plans: Investigate and fix error (see notes/misc.txt) that I think is related to the auto computation of encoder output dim. Train a model with this command:
python bin/s01-train-unsup-single-input.py --global_rand_p .5 --enc_kwargs "{pretrained: False}" --head_kwargs "{ps: .4}" --lrs 2e-4

9/19/20
-------
Progress: Found and fixed issue from yesterday (s01 was constructing head before initializing overall model so I was losing the part where I chopped off the pooling layer and it was attempting to pool again after my manually added pooling layer). Wrote wrapper to automatically chop off these layers. Added attribute to my encoder to store last output dim and updated s01 to check for this before using n_channels_out(). Launched training run v4. Updated gpu_setup() to throw warnings instead of errors because we want this to work on cpu too.

Plans: Examine results of training runs so far to try to form a strategy for next few training runs. Launch another one (maybe using my encoder since that showed some promise, or try a huge torchvision model to see if that's a promising knob to adjust). Should first investigate my encoder's model size/# of params vs. mobilenet. Consider doing some planning to timebox experiments for the unsupervised model - at some point I need to try transferring the weights, which is basically the whole point.

9/20/20
-------
Progress: Read up a bit on different torchvision model architectures to see what bigger/better alternatives to mobilenet we have there. Updated TorchvisionEncoder to work with several different model families (realized only mobilenet and densenet use the 'features' child name. We now support mobilenet, densenet, resnet, and resnext variants). Wrote docs for n_out_channels(). V5 training run to try to address v4's underfitting. Snuck in another v6 run trying to address v5's overfitting.

Plans: Examine V5 training run. Start considering how supervised training will work (i.e. can I reuse s01?). Start working on adapting s01, or writing a new script if necessary. Update s01 earlystopper callback to allow using val_acc instead of val_loss.

9/21/20
-------
Progress: Examined v5 and v6 training runs. Added `numel` method to incendio base model. Examined model sizes (param counts, sys size, GPU memory) of different resnext arches. Launched v7 with giant resnext model and higher random noise probability. Started porting lightning supervised datamodule to dataset that will work with my get_databunch function (think I can repurpose s01 for supervised training). Added option in s01 to use a different metric than loss as an early stopping criteria.

Plans: Assess results of v7. Finish porting supervised dataset. Start updating get_databunch function and/or s01 to accomodate it.

9/22/20
-------
Progress: Examined v7 run and wrote some notes. Finished supervised dataset. Updated get_databunch to work with it. Tested this in notebook and seems to work as expected. Fiddled around a bit trying to overwrite `self` to get subsetting functionality working but ultimately realized it was possible to change the `samples` attr.

Plans: Update s01 to accomodate supervised training.

9/23/20
-------
Progress: Wrote function to load encoder weights after much debugging (might need to make this a little more generalizable at some point but fine for now). Briefly started updating s01 for supervised training. Also updated incendio modelcheckpoint callback to allow customizing file names.

Plans: Continue updating s01 for supervised training.

9/24/20
-------
Progress:

Plans:


