7/30/20
-------
Progress: Brainstormed ideas for intial approaches to imagewang.

Plans: Write detailed pre-mortem. Create github repo and push current files.

7/31/20
-------
Progress: Wrote plans and pre-mortem. Created github repo and pushed files. Created new Comet.ml project. Played a few games at http://www.happy-neuron.com/ to generate ideas for visual reasoning and get other ideas for learning.

Plans: Try loading a few sample images to get a feel for dir structure and get a sense of what's in our dataset, how big each split is, etc. Start following along with epoching blog (leaning towards starting with my mixup-inspired task).


8/1/20
-------
Progress: Fiddled around a bit with fastai's datablocks and lower level API but concluded pytorch is more intuitive, more fun, and should work fine. As long as I make everything work with the fastai training loop I think I'll consider that requirement met. Started working on torch dataset, mostly consisting of transform that mixes 2 images (functionality isn't very generalized yet).

Plans: Fix tensor resize issue in dataset. Once I have things working in pure torch, maybe go back and take another stab at the fastai way.


8/2/20
-------
Progress: Added image resize using native Pillow method. Batching wasn't collating right so struggled a bit with custom collate_fn's, but ultimately changed dataset getitem to make this easier. Started checking how to re-separate batch into the desired parts for the model (i.e. need x_constructed, x_a, x_b, x_c, where each x is shape (bs, channels, h, w).

Plans: Start working on model. Mostly just make sure batching is set up in a usable way that will allow me to encode each image separately.

8/3/20
-------
Progress: Cleaned up MixupTransform and made it a little more generalizable. Ported data functions/classes to lib and wrote docs. Started very early work on model. Fixed issue loading 1 channel images.

Plans: Take another look at fastai blog post. Consider best way to deal with encoding multiple images (maybe simplest to keep source images horizontally stacked as single tensor?). Work on building encoder.

8/4/20
-------
Progress: Figured out weird dataset/dataloader batching issue and changed it to return each image separately instead of concatenating into 1 giant image (cause: getitem should return like (x1, x2, x3, x4, y), not ((x1, x2, x3, x4), y)). Built out first stab at model arch using a stack of conv blocks.

Plans: Update incendio resblock interface to be consistent with convblock (seems like I'm leaning towards using that over fastai. I guess that wouldn't be so bad - using fastai does risk me glossing over some details, and if this leads to more comfort with torch and a more developed incendio library, that would be good too). Generalize UnmixerModel to accept resblock. Also consider: did I make a mistake by restructuring the batch this way instead of applying an encoder n+1 times? This is just the pre-training task, eventually I'll want to input only 1 image at a time. Maybe it's okay, just need to give this some thought.

8/5/20
-------
Progress: Gave some thought to the issue of processing all images at once and concluded that it's probably fine - when transferring, I can just pull out self.conv and self.pool and add a new head in forward(). Updated incendio resblock and convblock interfaces to be more similar. Updated UnmixerModel interface to allow the addition of 1 or more resblocks after the initial conv blocks (realized it might be unwise to use res blocks from the start since we can't increase the channel dimension) and generally did a pretty significant model overhaul. Wrote wrapper to get train/val datasets and dataloaders. Wrote dataset shuffling callback. Fixed a few incendio bugs and started training on a tiny dataset for debugging purposes.

Plans: Investigate empty history plot bug from MetricHistory callback. Investigate if model is learning anything (early results look pretty random) and if not, try to determine the cause (e.g. is LR just too high or is something fundamentally wrong with the model, e.g. mixing data across batches or something).

8/6/20
-------
Progress: Found cause of history plot bug and fixed it (at least I think - haven't thoroughly tested cases where there are more metrics). Continued trying to overfit on tiny batch with smaller LR and more epochs and still failed. Think there must be a problem with the model, probably mixing up images or examples somewhere along the way. Started working on a model variation that foregoes complicated batching and just puts each image through the encoding layers separately.

Plans: Finish up simple encoder and see if that also fails to learn on a tiny batch.

8/7/20
-------
Progress: Finished simpler encoder. Found and fixed bug in dataset that was giving it the wrong length. Found and fixed bug in model forward where I was squeezing the batch dimension if it was 1. The model continues to fail to learn on even a tiny subset.

Plans: Try swapping in a large pre-trained model to see if that can learn. This doesn't count for the self-supervised task but it's a way to see if my model is the problem. If this still doesn't learn, maybe it's something with the dataset or dataloader or loss function. Or could the task just be too hard when training a model from scratch? That seems unlikely, unless maybe we just need a bigger model? 

8/8/20
------
Progress: Tested pre-trained mobilenet encoder on tiny subset and that also failed to learn. Built model wrapper that allows us to pass in an encoder network (e.g. mobilenet) and applies the additional pooling, dot products, softmax, etc. at the end. This should make it easier to swap different architectures in and out. Wrote 2 new datasets to allow more testing to determine where the bug is.

Plans: Test model on ScaleDataset and see if that is able to learn. If not, maybe there's a problem with my dot product to softmax steps. Clean up nb2 a bit.

8/9/20
------
Progress: Updated get_databunch() function to allow us to choose which dataset type to create and added extra defaults to shuffle train set and drop incomplete batches. Found bug where Unmixer referenced global var and removed it. Added sigmoid option as final activation since softmax seems somewhat problematic here. Trained a bit on scale dataset and model shows some ability to learn, though the sigmoid model still outputs ~1 for the nonzero images. Still not really learning on mixup task. I think my dot product layer needs to be fleshed out into a more powerful classification head.

Plans: Refactor Unmixer to accept both the encoder and decoder as separate modules and build current functionality into a new decoder class. Maybe try a couple other decoder options (probably just start with some additional linear layers of some sort - have to think about exact strategy and implementation).

8/10/20
------
Progress: Heavy refactoring: Unmixer now accepts an encoder and a classification head. Built abstract classification head class, refactored dotproducthead into its own class, and added MLPhead class. Updated SmoothSoftmax class from attentive pooling experiments. Tried it out briefly and I THINK everything looks okay but need to check more. Trained on subset with MLP head and sigmoid activation and net now predicts all zeros :( .

Plans: Clean up notebook - it's gotten out of hand. Think it may be nearing the time to port some functionality to lib and start a fresh nb. Maybe first experiment a tiny bit more to see if a softmax last_act works better.

8/11/20
------
Progress: Tried a couple variations of refactored models on ScaleDataset and now the model completely fails to learn (guess I should have kept that example that was working). Ported all the models to lib and wrote some documentation, though skipped the more parameter-heavy constructors: it's possible we'll need to refactor more so I'm holding off on that. Part of the goal here was just to put everything in pycharm to see if it found any dumb global variable mistakes (it did not).

Plans: Start new clean notebook. Go through notes and try to reproduce the 1 or 2 examples that sort of worked. If necessary, create an even simpler version of ScaleDataset and see if I can get that to work (e.g. n=1 so we just have the original image and 1 scaled version? Have to see if this is compatible with current implementation).


8/12/20
------
Progress: Found and fixed bug in DotProductHead where last_act was applied (it's applied again by parent class). Changed Unmixer to inherit from BaseModel. Updated ScaleDataset to allow for n as low as 1. Ran some more experiments and still failed to learn anything - in fact, it's not clear if model weights are even changing. Predictions seem to remain pretty constant, which I initially thought might be caused by one of the models referencing some global variable. Now I'm wondering if I messed up the incendio training loop or optimizer with one of my recent changes. Explored different loss functions a bit and realized these probably aren't really itended for vector regression (may not be handling batches right).

Plans: Investigate whether weights are changing (aggregate stats seem unchanged but maybe specific weights change). Maybe best way forward is to just bite the bullet and write a pure torch loop to see if incendio is the problem. Or maybe I can try Lightning without rewriting anything. I've already scaled data/task complexity way down so next is to rule out incendio bugs.

8/13/20
-------
Progress: Ported functionality to pytorch lightning module and datamodule. Tried creating cometml logger but was getting errors so tried out mlflow logger (still some kinks to work out). Finding it surprisingly hard to find metric histories, especially train metrics - maybe need to explicitly add that to the logger in forward_step(). Early results don't look encouraing though (based on glimpses of loss in tqdm progress bar). This suggests the bug is not with the training loop.

Plans: Continue testing with lightning, adding ways to see training metrics. Try replacing variable LR optimizer with default torch optim to rule out that as a cause. Try replacing model with something SUPER simple. Starting to wonder if it's the loss function, but it may be a good idea to rule out a couple other causes first. Try to get lightning + mlflow + comet to save everything where I want it rather than cluttering up root directory.

8/14/20
-------
Progress: Built new dataModule and model for supervised task. Swapped out repeated squeeze calls in unsupervised model for a PoolFlatten layer. Tested supervised data/model on a subset and I think there's STILL an error (though hard to confirm since lightning seems intent on hiding all metrics).

Plans: Figure out where lightning stores training metrics. Try to identify possible bug in supervised task code (hoping to succeed in overfitting on 1 batch).

8/15/20
-------
Progress: Realized I was using wrong loss for supervised task and swapped in F.cross_entropy. Wrote function to create a subset of a built-in Torch dataset (haven't tested it yet on my custom datasets but I think it's flexible enough that it should be usable with them). Succeeded in overfitting on a tiny subset of the supervised task using Incendio (1 class only), then confirmed training also seems to be working on a slightly larger subset representing most classes (40 row train set). Ported ds_subset function to lib and wrote docstring.

Plans: Now that incendio loop/optimizer seems unlikely to have caused the failure to learn on the unsupervised task, investigate loss function. See if there's something built-in I should be using (maybe pairwise_distance will work?) or write wrapper to L2/L1 loss that handles reductions appropriately. Maybe port some of the recent lightning functionality to lib.


8/16/20
-------
Progress: Added multi-label classification mode to mixup and scale datasets (simpler task: predict which images are sources, not what their weights were). This lets us train in a way that I'm confident is not mis-using a loss function (binary_cross_entropy_with_logits with OHE targets). Confirmed ability to overfit 1 batch using identity last_act, 3 layer encoder, MLP head. Had to restart nb due to autoreload failure and training is now oddly much slower. Maybe I was using dot product head before, but encoder was much bigger. Tried classification mode with MixupDS and model predicted ~.67 for everything.

Plans: Think about what this problem means and research fixes. Or port lightning functionality and write some documentation if I want something less mentally taxing.

8/17/20
-------
Progress: Laid out some thoughts on different approaches to solve problem and decided to work on regression version of ScaleDS. Wrote loss wrapper for pairwise distance and trained a bit in regression mode - model seems to learn a little bit but still struggles to overfit one batch. Maybe need bigger model/more epochs/lower LR? Ported some lightning functionality to lib. Started looking through siamese network repo for ideas on what could be wrong.

Plans: Investigate more to see if scaleDS regression is actually learning (may want to break out GPU for larger/longer training - should still be extremely fast on 1 batch, and it's been getting annoyingly slow on CPU as I increase model size). Less mentally taxing alternative: still tons of documentation to write. Also can look more through siamese network repo.

8/18/20
-------
Progress: Added documentation for datasets and models (lightning modules and data modules are still undocumented).

Plans: Same as today: probably just write lightning documentation and/or read through siamese network repo. If I have more energy, launch larger scaleDS regression run to see if learning is taking place.

8/19/20
-------
Progress: Read a bit about contrastive loss and took first stab at implementing it. Have a functional version and an OOP version, both of which seem to work, but some work remains.

Plans: Read more about contrastive loss variant that uses softmax and cosine distance and try implementing that (maybe make it so there's 1 top level interface but the user can specify distance='cosine' or mode='softmax' or something). If that gets done, could look into adapting this for use on soft targets.

8/20/20
-------
Progress: Read a little about contrastive loss variant. Ended up deciding that with my current setup, this would work best split between a very simple classification head that computes the cosine similarity (or other metric if desired), leaves the temperature scaling and log softmax to the last activation, and uses NLLLoss as the loss function. Ended up refactoring smoothsoftmax to provide a logsoftmax option.

Plans: Update ClassificationHead (and other classes if necessary) to accomodate log variant of smoothsoftmax (realized torch losses expect either logits or logsoftmax outputs). Maybe return to contrastive loss issues (see nb04 todo).

8/21/20
-------
Progress: Updated classification head to allow log softmax. Built contrastiveloss2d variant that seems to be working as I envisioned when comparing 1 image to multiple others. Wrote just enough documentation to remember what I'm doing but it still needs to be cleaned up.

Plans: Options depending on energy level. Easy task: port and document contrastive loss 1d/2d/functional forms. Medium task: work on contrastive loss variant for scalar inputs (maybe this already works? As in works from a python standpoint, no idea if this will work well in practice from an ML standpoint). Harder task: return to the problem of scaleDS regression by training a larger model for longer on paperspace now that wifi's working again.

8/22/20
-------
Progress:

Plans: 



