7/30/20
-------
Progress: Brainstormed ideas for intial approaches to imagewang.

Plans: Write detailed pre-mortem. Create github repo and push current files.

7/31/20
-------
Progress: Wrote plans and pre-mortem. Created github repo and pushed files. Created new Comet.ml project. Played a few games at http://www.happy-neuron.com/ to generate ideas for visual reasoning and get other ideas for learning.

Plans: Try loading a few sample images to get a feel for dir structure and get a sense of what's in our dataset, how big each split is, etc. Start following along with epoching blog (leaning towards starting with my mixup-inspired task).


8/1/20
-------
Progress: Fiddled around a bit with fastai's datablocks and lower level API but concluded pytorch is more intuitive, more fun, and should work fine. As long as I make everything work with the fastai training loop I think I'll consider that requirement met. Started working on torch dataset, mostly consisting of transform that mixes 2 images (functionality isn't very generalized yet).

Plans: Fix tensor resize issue in dataset. Once I have things working in pure torch, maybe go back and take another stab at the fastai way.


8/2/20
-------
Progress: Added image resize using native Pillow method. Batching wasn't collating right so struggled a bit with custom collate_fn's, but ultimately changed dataset getitem to make this easier. Started checking how to re-separate batch into the desired parts for the model (i.e. need x_constructed, x_a, x_b, x_c, where each x is shape (bs, channels, h, w).

Plans: Start working on model. Mostly just make sure batching is set up in a usable way that will allow me to encode each image separately.

8/3/20
-------
Progress: Cleaned up MixupTransform and made it a little more generalizable. Ported data functions/classes to lib and wrote docs. Started very early work on model. Fixed issue loading 1 channel images.

Plans: Take another look at fastai blog post. Consider best way to deal with encoding multiple images (maybe simplest to keep source images horizontally stacked as single tensor?). Work on building encoder.

8/4/20
-------
Progress: Figured out weird dataset/dataloader batching issue and changed it to return each image separately instead of concatenating into 1 giant image (cause: getitem should return like (x1, x2, x3, x4, y), not ((x1, x2, x3, x4), y)). Built out first stab at model arch using a stack of conv blocks.

Plans: Update incendio resblock interface to be consistent with convblock (seems like I'm leaning towards using that over fastai. I guess that wouldn't be so bad - using fastai does risk me glossing over some details, and if this leads to more comfort with torch and a more developed incendio library, that would be good too). Generalize UnmixerModel to accept resblock. Also consider: did I make a mistake by restructuring the batch this way instead of applying an encoder n+1 times? This is just the pre-training task, eventually I'll want to input only 1 image at a time. Maybe it's okay, just need to give this some thought.

8/5/20
-------
Progress: Gave some thought to the issue of processing all images at once and concluded that it's probably fine - when transferring, I can just pull out self.conv and self.pool and add a new head in forward(). Updated incendio resblock and convblock interfaces to be more similar. Updated UnmixerModel interface to allow the addition of 1 or more resblocks after the initial conv blocks (realized it might be unwise to use res blocks from the start since we can't increase the channel dimension) and generally did a pretty significant model overhaul. Wrote wrapper to get train/val datasets and dataloaders. Wrote dataset shuffling callback. Fixed a few incendio bugs and started training on a tiny dataset for debugging purposes.

Plans: Investigate empty history plot bug from MetricHistory callback. Investigate if model is learning anything (early results look pretty random) and if not, try to determine the cause (e.g. is LR just too high or is something fundamentally wrong with the model, e.g. mixing data across batches or something).

8/6/20
-------
Progress: Found cause of history plot bug and fixed it (at least I think - haven't thoroughly tested cases where there are more metrics). Continued trying to overfit on tiny batch with smaller LR and more epochs and still failed. Think there must be a problem with the model, probably mixing up images or examples somewhere along the way. Started working on a model variation that foregoes complicated batching and just puts each image through the encoding layers separately.

Plans: Finish up simple encoder and see if that also fails to learn on a tiny batch.

8/7/20
-------
Progress: Finished simpler encoder. Found and fixed bug in dataset that was giving it the wrong length. Found and fixed bug in model forward where I was squeezing the batch dimension if it was 1. The model continues to fail to learn on even a tiny subset.

Plans: Try swapping in a large pre-trained model to see if that can learn. This doesn't count for the self-supervised task but it's a way to see if my model is the problem. If this still doesn't learn, maybe it's something with the dataset or dataloader or loss function. Or could the task just be too hard when training a model from scratch? That seems unlikely, unless maybe we just need a bigger model? 

8/8/20
------
Progress: Tested pre-trained mobilenet encoder on tiny subset and that also failed to learn. Built model wrapper that allows us to pass in an encoder network (e.g. mobilenet) and applies the additional pooling, dot products, softmax, etc. at the end. This should make it easier to swap different architectures in and out. Wrote 2 new datasets to allow more testing to determine where the bug is.

Plans: Test model on ScaleDataset and see if that is able to learn. If not, maybe there's a problem with my dot product to softmax steps. Clean up nb2 a bit.

8/9/20
------
Progress: Updated get_databunch() function to allow us to choose which dataset type to create and added extra defaults to shuffle train set and drop incomplete batches. Found bug where Unmixer referenced global var and removed it. Added sigmoid option as final activation since softmax seems somewhat problematic here. Trained a bit on scale dataset and model shows some ability to learn, though the sigmoid model still outputs ~1 for the nonzero images. Still not really learning on mixup task. I think my dot product layer needs to be fleshed out into a more powerful classification head.

Plans: Refactor Unmixer to accept both the encoder and decoder as separate modules and build current functionality into a new decoder class. Maybe try a couple other decoder options (probably just start with some additional linear layers of some sort - have to think about exact strategy and implementation).

8/10/20
------
Progress: 

Plans:




