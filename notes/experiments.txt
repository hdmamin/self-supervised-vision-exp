v0
--
Summary:
Pretrained encoder with single linear layer head, first experiment that's not on a tiny subset.

Approximate Stats:
GPU Memory Usage: 1,985/8,126 MB
GPU Utilization: big fluctuations between 0-98%, only high for brief moments
CPU Memory: 2,425/30,147 MB

Takeaways:
-bs of 64 could be much bigger. Experiment with a couple epoch run to see how big we can go.
-train loss is going down, though at first it didn't look like it. Take a look at metric plots afterwards to see how volatile things are: at first I thought LR was too high but now things are looking pretty slow. Maybe this is a good opportunity for 1 cycle scheduling: let it ramp up to the max LR. [UPDATE: loss curves actually look pretty good, not overly noisy or overly gradual.]
-add std_soft_prediction next time: want to make sure we're not predicting a constant.
-Remember this is still kind of a debugging run: we're using a pretrained encoder which is cheating. Maybe worth stopping this early.


v1
--
Summary:
Same as V0 (pretrained encoder with single linear layer head) but greatly increased bs from 64 to 512. Also added std_soft_predictions metric.

Approximate Stats:
GPU Memory Usage: 11,889/8,126 MB
GPU Utilization: Mostly either 0 or 100, I think because batches take a long time to load. Sort of though I remembered that torch loads next batch and stores in CPU until ready, but maybe that's an option I have to enable.
CPU Memory: 3,000/30,147 MB (max usage)

Takeaways:
-Epoch time decreased from ~1 minute to ~30 seconds with the bigger bs. If the "extra batch in CPU" option isn't enabled, this could likely be dramatially decreased.
        -Try num_workers>0 next time. I think this may achieve what I'm looking for.
-Realized in v0, val dataloader was dropping incomplete batches. Fixed this in lib.
-Looks like loss is going down more slowly on both train and val sets. Examine loss curves for more insight into this. Realized with more stable gradients from bigger bs, we should probably be upping the LR.

v2
--
Summary:
Used mobilenet arch with random weights for encoder to see if the success of v0/v1 were just due to transfer learning (result: looks like they were).

Takeaways:
-Look at epoching blog post again to see if they trained the encoder separately first (don't think they did).
-Noticed grads were very big for early layers of encoder and decreased as we got deeper into the network. Sort of the opposite of the classic vanishing gradient problem. Not sure what to make of this. Would grad clipping be useful here (prevent a few layers from overpowering the rest) or would this just remove the little signal we do have? Probably doesn't matter for this problem since it doesn't seem to be learning regardless, but I think we saw a similar issue before so this will likely crop up again.

v3
--
Summary:
Use my encoder instead of torchvision and fastai head instead of single linear layer. I'm thinking the classification layer didn't have enough "firepower" compared to the encoder. IIRC, this previously didn't work so well on a subset but I thought turning down dropout might help.

Takeaways:
-Turning down dropout did succeed in allowing us to fit the training set, but we still don't learn much on the validation set. Could try tweaking that parameter a bit.

v4
--
Summary:
First full run with s01 rather than nb. Used randomly initialized mobilenet encoder and fastai head with slightly higher dropout (p=.4). Also added all 3 random transforms with p=0.5.

Takeaways:
-Initially looked like it was learning nothing but after a couple dozen epochs it does seem like it's reliably staying above chance (though not by a whole lot). Still underfitting. Possibilities:
    -much larger model (inception, resnext?)
    -turn down dropout?
    -turn down transform probability (these are applied to train set but not valid set, I'm thinking that might explain why val stats are a little better. Though I'm not sure whether removing it would actually be useful.).

v5
--
Summary:
Use bigger model (resnext) and turned down augmentation and dropout to try to address V4's underfitting.

Approximate Stats:
GPU Memory Usage: 5,913/8,126 MB
GPU Utilization: mostly at 99%, sometimes drops to 50% but it's pretty brief
CPU Memory: up to ~3,630/30,147 MB (initially thought there was a memory leak since this seemed to keep increasing, but I think we're okay. Not sure exactly what was happening.)
Epoch time (train): ~70s

Takeaways:
-Overfittting now.

v6
--
Summary:
Now that v5 overfits, try turning up dropout and data augmentation.

Takeaways:
-Performed very slightly better, maybe not significantly so. Still overfitting.
-Could maybe increase early stopping patience.

v7
--
Summary:
Bump up to even bigger model, turn up random noise augmentation, and increase stopping patience in hopes that if we just keep training for a while, val loss might break through and improve again. We still haven't quite gotten to a place where even train performance is human level (though v6 is starting to get in the ballpark), so I'd like to try one last experiment with an enormous model and see if that helps at all. Regardless, I think it's getting to be time to wrap up this portion of the experiment.

Approximate Stats:
GPU Memory Usage: 4,667/8,126 MB
GPU Utilization: fluctuates 9-98% but doesn't stay low for long
CPU Memory: ~2720 MB
Epoch time (train): ~4:50

Takeaways:
-Looks like this did considerably better than the smaller resnext arch.
-Best model was from epoch 42 and last epoch was 66 (0-indexed). Is my early stopping callback counting off by 1, or maybe it stops training before the end of epoch MetricPrinter callback is called?
-This is the first model where I feel we really let the model reach high train acc and saw conclusive overfitting on val set.
-Kind of unrelated to our task, but it's pretty weird that overfitting on the train set resulted in predicting more and more positives on the val sset. Not sure what would explain that.

v8
--
Summary:
Channel shuffle task with randomly initialized mobilenet encoder.

Approximate Stats:
GPU Memory Usage: 
GPU Utilization: 
CPU Memory: 
Epoch time (train): 

Takeaways:
-Didn't record any stats or observations at the time, but I realized comet tracks some of these things (gpu utilization, time, etc.) Also looks like they may get CLI kwargs (similar to my log_cmd decorator) and stdout (though tqdm bar causes problems here). I think it's still nice to have some of these things locally though. 
-Performance was quite good and maybe still underfitting (prob should have monitored acc instead of loss). That doesn't mean the task is useful - it could be too trivial or niche - but at least the implementation seems to work ok.

-----------------
SUPERVISED MODELS
-----------------

v0
--
Summary:
First full supervised training run. Use large resnext with random weights so we can compare to later runs with pretrained encoder weights.

Approximate Stats:
GPU Memory Usage: 4,667/8,126 MB
GPU Utilization: fluctuates 10-98% but doesn't stay low for long
CPU Memory: ~3330 MB
Epoch time (train): ~10min

Takeaways:
-First attempt had massive negative loss, stopped training manually. Fixed several bugs (mostly holdovers from assumption that script was for unsupervised task, particularly binary classification: wrong loss function, wrong # of output classes, wrong shape and dtype of labels, wrong trainer.last_act, wrong trainer.mode).
-Okay train performance, useless on validation set though.

v1
--
Summary:
Same as v0 but with pretrained encoder.

Approximate Stats:
GPU Memory Usage: 4,723/8,126 MB
GPU Utilization: fluctuates 10-98%, doesn't stay low for long but definitely not steadily high either
CPU Memory: ~3,320 MB
Epoch time (train): ~10min

Takeaways:
-Validation set performance is still no better than chance. How can model be that bad? Even with no pre-training, we really shouldn't be doing THAT poorly. Maybe resnext is so giant it really needs to be pretrained otherwise it overfits too easily?

v2
--
Summary:
Same as v1 but freeze encoder for first 3 epochs and slightly increase dropout in classification head.

Approximate Stats:
GPU Memory Usage: 1,1827/8,126 MB when training just head
GPU Utilization: fluctuates 90-98%
CPU Memory: ~2,958 MB when training just head
Epoch time (train): ~2:10min when training just head

Takeaways:
-

v3
--
Summary:
Re-doing v0 after fixing bug in supervised databunch: resnext with no pretrained weights (either imagenet or SSL).

Approximate Stats:
GPU Memory Usage: 4,667/8,126 MB
GPU Utilization: fluctuates 10-98% but doesn't stay low for long
CPU Memory: ~4580 MB
Epoch time (train): ~10min

Takeaways:
-Interesting that this takes up more cpu memory than v0. Wonder if it just fluctuates a lot and I happened to check it at different times? Didn't seem like it though.


v4
--
Summary:
Re-doing v0 after REALLY fixing bug in supervised databunch: resnext with no pretrained weights (either imagenet or SSL).

Approximate Stats:
GPU Memory Usage: 4,667/8,126 MB
GPU Utilization: fluctuates 10-98%, doesn't stay constant for long
CPU Memory: ~3,110 MB
Epoch time (train): ~10min

Takeaways:
-Still performing at chance on validation set.

v5
--
Summary:
Supervised training with pretrained mobilenet. Faster to experiment with than resnext and want to use pretrained weights to see if we still perform at chance. Also speculating maybe resnext is just so big that is massively overfits? Still seems crazy that validation performance could be so bad though.

Approximate Stats:
GPU Memory Usage: 1,239/8,126 MB
GPU Utilization: fluctuates 63-73%
CPU Memory: ~3,110 MB
Epoch time (train): ~1min

Takeaways:
-Forgot to increase batch size when reverting to mobilenet.
-Realizing this is meant to be a difficult task, hence the need for SSL (if training from scratch worked well, there'd be no need). Not sure if transfer learning is expected to work well here or not - in some situations it's not an option (probably no massive pretrained models for some specific types of medical images) so I don't know what to make of this result. Imagewang leaderboard tops out around ~65% accuracy for this size of image which doesn't sound amazing, but that's actually doing better than my pretrained imagenet models :| .

v6
--
Summary:
Same as v5 but with random weights. 

Approximate Stats [NOTE: didn't record gpu utilization, just leaving v5's stats there since it's probably similar]:
GPU Memory Usage: 1,239/8,126 MB
GPU Utilization: fluctuates 63-73%
CPU Memory: ~2,920 MB
Epoch time (train): ~1min

Takeaways:
-v6's val acc was still consistently improving but loss wasn't so it was stopped prematurely.

v7
--
Summary:
Same as v6 but monitor accuracy instead of loss for early stopping 

Approximate Stats:
GPU Memory Usage: 3,361/8,126 MB
GPU Utilization: fluctuates 73-100%
CPU Memory: ~3,170 MB
Epoch time (train): ~45s

Takeaways:
-Bug in s01, needed to update earlystopper callback when monitoring different quantity. >:(

v8
--
Summary:
Mobilenet w/ random weights.

Approximate Stats:
GPU Memory Usage: 
GPU Utilization: 
Epoch time (train): ~15s when head is frozen, ~45s afterwards

Takeaways:
CPU Memory: 
-Nearly 0% accuracy on validation set??? [UPDATE: not sure what I was looking at. Looks like val acc got up to ~27%, still not great but def better than chance.]

v9
--
Summary:
Try loading weights from mobilenet SSL v4.

Approximate Stats:
GPU Memory Usage: 
GPU Utilization: 
CPU Memory: 
Epoch time (train): ~15s when head is frozen, ~45s afterwards

Takeaways:
-Nearly 0% accuracy on validation set???

v10
---
Summary:
Try loading weights from mobilenet SSL v8 (channel shuffle).

Approximate Stats:
GPU Memory Usage: 
GPU Utilization: 
CPU Memory: 
Epoch time (train): ~15s when head is frozen, ~45s afterwards

Takeaways:
-8% val accuracy.

v11
---
Summary:
Transfer from vertical flip task. Also tried unfreezing the encoder a bit earlier and using higher encoder LR than before.

Takeaways:
-8% val accuracy.

