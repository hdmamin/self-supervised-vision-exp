v0
--
Summary:
Pretrained encoder with single linear layer head, first experiment that's not on a tiny subset.

Approximate Stats:
GPU Memory Usage: 1,985/8,126 MB
GPU Utilization: big fluctuations between 0-98%, only high for brief moments
CPU Memory: 2,425/30,147 MB

Takeaways:
-bs of 64 could be much bigger. Experiment with a couple epoch run to see how big we can go.
-train loss is going down, though at first it didn't look like it. Take a look at metric plots afterwards to see how volatile things are: at first I thought LR was too high but now things are looking pretty slow. Maybe this is a good opportunity for 1 cycle scheduling: let it ramp up to the max LR. [UPDATE: loss curves actually look pretty good, not overly noisy or overly gradual.]
-add std_soft_prediction next time: want to make sure we're not predicting a constant.
-Remember this is still kind of a debugging run: we're using a pretrained encoder which is cheating. Maybe worth stopping this early.


v1
--
Summary:
Same as V0 (pretrained encoder with single linear layer head) but greatly increased bs from 64 to 512. Also added std_soft_predictions metric.

Approximate Stats:
GPU Memory Usage: 11,889/8,126 MB
GPU Utilization: Mostly either 0 or 100, I think because batches take a long time to load. Sort of though I remembered that torch loads next batch and stores in CPU until ready, but maybe that's an option I have to enable.
CPU Memory: 3,000/30,147 MB (max usage)

Takeaways:
-Epoch time decreased from ~1 minute to ~30 seconds with the bigger bs. If the "extra batch in CPU" option isn't enabled, this could likely be dramatially decreased.
        -Try num_workers>0 next time. I think this may achieve what I'm looking for.
-Realized in v0, val dataloader was dropping incomplete batches. Fixed this in lib.
-Looks like loss is going down more slowly on both train and val sets. Examine loss curves for more insight into this. Realized with more stable gradients from bigger bs, we should probably be upping the LR.

v2
--
Summary:
Used mobilenet arch with random weights for encoder to see if the success of v0/v1 were just due to transfer learning (result: looks like they were).

Takeaways:
-Look at epoching blog post again to see if they trained the encoder separately first (don't think they did).
-Noticed grads were very big for early layers of encoder and decreased as we got deeper into the network. Sort of the opposite of the classic vanishing gradient problem. Not sure what to make of this. Would grad clipping be useful here (prevent a few layers from overpowering the rest) or would this just remove the little signal we do have? Probably doesn't matter for this problem since it doesn't seem to be learning regardless, but I think we saw a similar issue before so this will likely crop up again.


v3
--
Summary:
Use my encoder instead of torchvision and fastai head instead of single linear layer. I'm thinking the classification layer didn't have enough "firepower" compared to the encoder. IIRC, this previously didn't work so well on a subset but I thought turning down dropout might help.

Takeaways:
-Turning down dropout did succeed in allowing us to fit the training set, but we still don't learn much on the validation set. Could try tweaking that parameter a bit.

V4
--
Summary:
First full run with s01 rather than nb. Used randomly initialized mobilenet encoder and fastai head with slightly higher dropout (p=.4). Also added all 3 random transforms with p=0.5.

Takeaways:
-Initially looked like it was learning nothing but after a couple dozen epochs it does seem like it's reliably staying above chance (though not by a whole lot). Still underfitting. Possibilities:
    -much larger model (inception, resnext?)
    -turn down dropout?
    -turn down transform probability (these are applied to train set but not valid set, I'm thinking that might explain why val stats are a little better. Though I'm not sure whether removing it would actually be useful.).

V5
--
Summary:
Use bigger model (resnext) and turned down augmentation and dropout to try to address V4's underfitting.

Approximate Stats:
GPU Memory Usage: 5,913/8,126 MB
GPU Utilization: mostly at 99%, sometimes drops to 50% but it's pretty brief
CPU Memory: up to ~3,630/30,147 MB (initially thought there was a memory leak since this seemed to keep increasing, but I think we're okay. Not sure exactly what was happening.)
Epoch time (train): ~70s

Takeaways:
-Overfittting now.

V6
--
Summary:
Now that v5 overfits, try turning up dropout and data augmentation.

Takeaways:
-Performed very slightly better, maybe not significantly so. Still overfitting.
-Could maybe increase early stopping patience.

V7
--
Summary:
Bump up to even bigger model, turn up random noise augmentation, and increase stopping patience in hopes that if we just keep training for a while, val loss might break through and improve again. We still haven't quite gotten to a place where even train performance is human level (though v6 is starting to get in the ballpark), so I'd like to try one last experiment with an enormous model and see if that helps at all. Regardless, I think it's getting to be time to wrap up this portion of the experiment.

Approximate Stats:
GPU Memory Usage: 4,667/8,126 MB
GPU Utilization: fluctuates 9-98% but doesn't stay low for long
CPU Memory: ~2720 MB
Epoch time (train): ~4:50

Takeaways:
-Looks like this did considerably better than the smaller resnext arch.
-Best model was from epoch 42 and last epoch was 66 (0-indexed). Is my early stopping callback counting off by 1, or maybe it stops training before the end of epoch MetricPrinter callback is called?
-This is the first model where I feel we really let the model reach high train acc and saw conclusive overfitting on val set.
-Kind of unrelated to our task, but it's pretty weird that overfitting on the train set resulted in predicting more and more positives on the val sset. Not sure what would explain that.
